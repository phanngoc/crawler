{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl vnexpress.net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-18 16:26:00+07:00\n"
     ]
    }
   ],
   "source": [
    "import dateparser\n",
    "\n",
    "text = 'Thứ hai, 18/9/2023, 16:26 (GMT+7)'\n",
    "date = dateparser.parse(text)\n",
    "print(date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start:crawl_html: https://vnexpress.net/kinh-doanh-p1 {}\n",
      "Title: \n",
      "Được công nhận là nền kinh tế thị trường ý nghĩa gì với Việt Nam? \n",
      "Link: https://vnexpress.net/duoc-cong-nhan-la-nen-kinh-te-thi-truong-y-nghia-gi-voi-viet-nam-4656643.html\n",
      "get_page_detail https://vnexpress.net/duoc-cong-nhan-la-nen-kinh-te-thi-truong-y-nghia-gi-voi-viet-nam-4656643.html {}\n"
     ]
    }
   ],
   "source": [
    "from calendar import c\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import crawl\n",
    "from datetime import datetime, timedelta\n",
    "import dateparser\n",
    "import importlib\n",
    "importlib.reload(crawl)\n",
    "\n",
    "\n",
    "class VnexpressCrawler():\n",
    "    base_url = 'https://vnexpress.net'\n",
    "    page = 1\n",
    "    daily = False # if exist in database will kill process.\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_response(self, url, params = {}):\n",
    "        headers = {\n",
    "            'Access-Control-Allow-Origin': '*',\n",
    "            'Access-Control-Allow-Credentials': 'true',\n",
    "            'Access-Control-Allow-Methods': 'GET, HEAD, POST, PUT, DELETE, TRACE, OPTIONS, PATCH',\n",
    "            'Access-Control-Allow-Headers': \"\"\"X-Real-IP,X-AGENT,Pragma,X-REFERER,X-AUTH-TOKEN,Accept-Encoding,channel,X-XSS-Protection,X-Content-Type-Options,Strict-Transport-Security,Content-Type,Authorization,Accept,Origin,User-Agent,DNT,Cache-Control,X-Mx-ReqToken,Keep-Alive,X-Requested-With,If-Modified-Since,token-id\"\"\",\n",
    "            'Content-Encoding': 'gzip'\n",
    "        }\n",
    "        # Make an HTTP GET request to the API endpoint using the requests library\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        return response\n",
    "    \n",
    "    def get_page_detail(self, url, params = {}):\n",
    "        if url.startswith(self.base_url) == False:\n",
    "            url = self.base_url + url\n",
    "\n",
    "        print('get_page_detail', url, params)\n",
    "        response = self.get_response(url, params)\n",
    "        if response.status_code // 100 == 2:\n",
    "            content_html = response.content\n",
    "            soup = BeautifulSoup(content_html, 'html.parser')\n",
    "            # Find the title element\n",
    "            publishdate_elem = soup.select_one('.header-content span.date')\n",
    "            if publishdate_elem:\n",
    "                publishDate = publishdate_elem.get_text()\n",
    "                publishDate = publishDate.strip()\n",
    "                publishDate = dateparser.parse(publishDate)\n",
    "            else:\n",
    "                publishDate = None\n",
    "\n",
    "            content_detail_elem = soup.select_one('article.fck_detail')\n",
    "\n",
    "            content_detail = ''\n",
    "            if content_detail_elem:\n",
    "                content_detail = content_detail_elem.get_text()\n",
    "\n",
    "            return {\n",
    "                'date': publishDate,\n",
    "                'content': content_detail\n",
    "            }\n",
    "        else:\n",
    "            # If unsuccessful, print the status code and reason for failure\n",
    "            print(f\"Request failed with status code {response.status_code}: {response.reason}\")\n",
    "            return {}\n",
    "\n",
    "    def crawl_html(self, url, params = {}):\n",
    "        print('start:crawl_html:', url, params)\n",
    "        response = self.get_response(url, params)\n",
    "        if response.status_code // 100 == 2:\n",
    "            content_html = response.content\n",
    "\n",
    "            # Parse the HTML using BeautifulSoup\n",
    "            soup = BeautifulSoup(content_html, 'html.parser')\n",
    "\n",
    "            # Find the title element\n",
    "            title_elements = soup.select('.title-news a')\n",
    "            if title_elements is None or len(title_elements) == 0:\n",
    "                return False\n",
    "\n",
    "            for title_element in title_elements:\n",
    "                title = title_element.get_text()\n",
    "                link = title_element['href']\n",
    "\n",
    "                print(\"Title:\", title)\n",
    "                print(\"Link:\", link)\n",
    "                data_detail = self.get_page_detail(link, {})\n",
    "                item = {\n",
    "                    'domain': 'https://vnexpress.net/kinh-doanh',\n",
    "                    'title': title,\n",
    "                    'url': link,\n",
    "                    'date': data_detail['date'] if 'date' in data_detail else None,\n",
    "                    'content': data_detail['content'] if 'content' in data_detail else None,\n",
    "                }\n",
    "\n",
    "                if self.daily:\n",
    "                    isDup = crawl.create_article(item, False)\n",
    "                    if isDup is False:\n",
    "                        return False\n",
    "                else:\n",
    "                    crawl.create_article(item, True)\n",
    "\n",
    "        else:\n",
    "            # If unsuccessful, print the status code and reason for failure\n",
    "            print(f\"Request failed with status code {response.status_code}: {response.reason}\")\n",
    "            return False\n",
    "\n",
    "    def run(self, page = {\"from\": None, \"to\": None}, daily = False):\n",
    "        self.daily = daily\n",
    "        page_from = 1 if page['from'] is None else page['from']\n",
    "        page_to = -1 if page['to'] is None else page['to']\n",
    "        while (True):\n",
    "            isSuccess = self.crawl_html('https://vnexpress.net/kinh-doanh-p' + str(page_from), {})\n",
    "            # break\n",
    "            if isSuccess == False or page_to == page_from:\n",
    "                break\n",
    "            page_from += 1\n",
    "\n",
    "\n",
    "VnexpressCrawler().run(daily=True)\n",
    "# obj.get_page_detail('/von-ngoai-bat-dau-tang-toc-185230818163354771.htm')\n",
    "# crawl_json('https://finfo-api.vndirect.com.vn/v4/news?q=newsType:company_report~locale:VN~newsSource:VNDIRECT&sort=newsDate:desc~newsTime:desc&size=20&page=3', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
